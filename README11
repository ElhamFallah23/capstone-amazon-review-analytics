# Amazon Review Analytics â€” Event-Driven End-to-End Data Platform

## ğŸ“Œ Project Overview

This project is a production-style, end-to-end cloud data platform designed to ingest, process, transform, and analyze Amazon product review data using modern Data Engineering best practices.

The platform provisions infrastructure using **Terraform (Infrastructure as Code)**, deploys via **GitHub Actions CI/CD with OIDC authentication**, processes data using **AWS Glue & Step Functions**, and delivers analytics-ready data in **Snowflake**, modeled with **dbt** and orchestrated via **Apache Airflow**.

The architecture follows production-grade principles:

- Infrastructure as Code (IaC)
- Secure CI/CD (OIDC, no hard-coded credentials)
- Event-driven processing
- Modular Terraform design
- Layered data modeling (raw â†’ stage â†’ mart)
- Automated metadata refresh in Snowflake

---

## ğŸ— Architecture Overview

### High-Level Data Flow


S3 (raw)

â†“

S3 Event â†’ EventBridge

â†“

Step Functions

â†“

Glue Job (Cleaning)

â†“

Glue Job (Flattening)

â†“

S3 (processed â€“ Parquet)

â†“

Snowflake External Stage

â†“

External Table (Auto Refresh)

â†“

dbt (staging & mart models)

â†“

Airflow orchestration

â†“

QuickSight dashboards



---
The system is fully automated and event-driven from ingestion to analytics.

---

## âš™ï¸ Tech Stack

### Cloud & Infrastructure
- AWS (S3, EventBridge, Glue, Lambda, SNS, Step Functions, IAM)
- Terraform (modular Infrastructure as Code)
- GitHub Actions (OIDC-based CI/CD)

### Data & Analytics
- Snowflake (External Stage, External Table, Storage Integration)
- dbt Core (staging & mart modeling)
- Apache Airflow (workflow orchestration)
- Amazon QuickSight (BI visualization)

---

## ğŸ§  Key Architectural Highlights

### ğŸ” Secure CI/CD with OIDC (No Hard-Coded Credentials)

Infrastructure deployment is performed via GitHub Actions using **OIDC-based authentication**, eliminating long-lived AWS credentials.

Benefits:

- No hard-coded secrets
- Short-lived access tokens
- Principle of least privilege
- Enterprise-grade CI/CD security model

---

### âš¡ Fully Event-Driven Processing

The ingestion pipeline is fully event-driven:

1. A new file is uploaded to **S3 (raw)**
2. S3 emits an event
3. EventBridge captures the event
4. EventBridge triggers Step Functions
5. Step Functions orchestrates Glue jobs (clean â†’ flatten)

This enables:

- Zero manual triggering
- Decoupled architecture
- Automatic pipeline execution
- Scalable design

---

### â„ï¸ Snowflake External Table with Automatic Refresh

Processed Parquet files are written to **S3 (processed)** by Glue.

Snowflake is configured using:

- Storage Integration (IAM-based access)
- External Stage
- External Table
- Auto-refresh via S3 Event Notifications

When new files arrive:

- S3 event triggers Snowflake metadata refresh
- External table updates automatically
- No manual `COPY INTO` required

This enables near real-time analytics readiness.

---

### ğŸ” Airflow Orchestration (Extensible Design)

Currently:

- Airflow DAG runs on a scheduled interval.

Future enhancement:

- Event-driven Airflow triggering via EventBridge/S3
- Fully reactive orchestration layer

This keeps the system extensible and production-ready.

---

### ğŸ— Infrastructure as Code (Full Automation)

All infrastructure components are provisioned via Terraform:

- AWS resources
- IAM roles & policies
- Snowflake database objects
- OIDC roles for CI/CD
- Event integrations

No manual setup is required once deployed.

---

## ğŸ“Š Data Modeling Strategy

The platform follows a layered modeling approach:

### Raw Layer
- Landing zone in S3 (JSON format)

### Processed Layer
- Cleaned & flattened data
- Stored in Parquet format for efficient querying

### Staging Layer (dbt)
- Source-aligned transformations
- Minimal business logic
- Clear column standardization

### Mart Layer (dbt)
- Aggregated business-ready datasets
- Clearly defined grain per model
- Designed for BI consumption

Each mart model explicitly defines its grain and transformation logic.

---

## ğŸš€ CI/CD Strategy

Infrastructure is deployed using GitHub Actions:

- `terraform-validate-fmt.yaml` â†’ Validates formatting and syntax
- `terraform-pr-plan.yaml` â†’ Runs Terraform plan on pull requests
- `terraform-post-merge-apply.yaml` â†’ Applies changes after merge

Remote state management:

- S3 backend
- DynamoDB state locking

Security:

- OIDC authentication
- No static AWS credentials

---

## ğŸ“‚ Repository Structure

.github/workflows/   # CI/CD pipelines for Terraform

infra/               # Modular Terraform infrastructure

dbt/                 # Staging and mart models

airflow/             # DAG definitions

docs/                # Detailed architecture & infra documentation


---

## ğŸ›  How to Run (Development Flow)

1. Initialize Terraform backend
2. Deploy infrastructure via GitHub Actions
3. Upload dataset to S3 raw bucket
4. S3 event triggers the pipeline automatically
5. Verify processed data in S3 processed layer
6. Query Snowflake external tables
7. Run dbt transformations
8. Trigger Airflow DAG (if required)
9. Explore dashboards in QuickSight

---

## ğŸ” Security Design

- OIDC-based GitHub authentication
- Least-privilege IAM policies
- Terraform remote state locking
- Snowflake IAM-based storage integration
- No hard-coded credentials

---

## ğŸ“ˆ Future Improvements

- Incremental dbt models
- Event-driven Airflow orchestration
- Data quality monitoring & alerting
- Cost monitoring & optimization
- CI pipeline for dbt testing
- Data catalog integration

---

## ğŸ¯ Project Goals

This project demonstrates:

- Cloud-native data architecture
- Event-driven system design
- Secure Infrastructure as Code
- Modern analytics engineering practices
- Production-style CI/CD implementation

It is designed not as a simple bootcamp exercise, but as a scalable, secure, and automation-first data platform.

## ğŸ“¬ About the Author

This project is built as part of a Data Engineering capstone bootcamp project using real-world best practices and cloud-native design.